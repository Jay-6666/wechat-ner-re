import torch
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline, AutoModel
import gradio as gr
import re
import os
import json
import chardet
from sklearn.metrics import precision_score, recall_score, f1_score
import time
from functools import lru_cache  # æ·»åŠ è¿™è¡Œå¯¼å…¥
# ======================== æ•°æ®åº“æ¨¡å— ========================
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from contextlib import contextmanager
import logging
import networkx as nx
from pyvis.network import Network
import pandas as pd
import matplotlib.pyplot as plt

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ä½¿ç”¨SQLAlchemyçš„è¿æ¥æ± æ¥ç®¡ç†æ•°æ®åº“è¿æ¥
DATABASE_URL = "mysql+pymysql://user:password@host/dbname"  # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹è¿æ¥å­—ç¬¦ä¸²

# åˆ›å»ºå¼•æ“ï¼ˆè¿æ¥æ± ï¼‰
engine = create_engine(DATABASE_URL, pool_size=10, max_overflow=20, echo=True)

# åˆ›å»ºsessionç±»
Session = sessionmaker(bind=engine)

@contextmanager
def get_db_connection():
    """
    ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨è·å–æ•°æ®åº“è¿æ¥
    """
    session = None
    try:
        session = Session()  # ä»è¿æ¥æ± ä¸­è·å–ä¸€ä¸ªè¿æ¥
        logging.info("âœ… æ•°æ®åº“è¿æ¥å·²å»ºç«‹")
        yield session  # ä½¿ç”¨sessionè¿›è¡Œæ•°æ®åº“æ“ä½œ
    except Exception as e:
        logging.error(f"âŒ æ•°æ®åº“æ“ä½œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        if session:
            session.rollback()  # å›æ»šäº‹åŠ¡
    finally:
        if session:
            try:
                session.commit()  # æäº¤äº‹åŠ¡
                logging.info("âœ… æ•°æ®åº“äº‹åŠ¡å·²æäº¤")
            except Exception as e:
                logging.error(f"âŒ æäº¤äº‹åŠ¡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            finally:
                session.close()  # å…³é—­ä¼šè¯ï¼Œé‡Šæ”¾è¿æ¥
                logging.info("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")

def save_to_db(table, data):
    """
    å°†æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“
    :param table: è¡¨å
    :param data: æ•°æ®å­—å…¸
    """
    try:
        valid_tables = ["entities", "relations"]  # åªå…è®¸ä¿å­˜åˆ°è¿™äº›è¡¨
        if table not in valid_tables:
            raise ValueError(f"Invalid table: {table}")
        
        with get_db_connection() as conn:
            if conn:
                # è¿™é‡Œçš„æ“ä½œå‡è®¾ä½¿ç”¨äº†ORMæ¨¡å‹æ¥å¤„ç†æ’å…¥ï¼Œå®é™…æ ¹æ®ä½ æ•°æ®åº“çš„è¡¨ç»“æ„æ¥è°ƒæ•´
                table_model = get_table_model(table)  # å‡è®¾ä½ æœ‰ä¸€ä¸ªæ–¹æ³•æ¥æ ¹æ®è¡¨åè·å¾—ORMæ¨¡å‹
                new_record = table_model(**data)
                conn.add(new_record)
                conn.commit()  # æäº¤äº‹åŠ¡
    except Exception as e:
        logging.error(f"âŒ ä¿å­˜æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False
    return True

def get_table_model(table_name):
    """
    æ ¹æ®è¡¨åè·å–ORMæ¨¡å‹ï¼ˆè¿™é‡Œå‡è®¾ä½ æœ‰ä¸€ä¸ªæ˜ å°„åˆ°æ•°æ®åº“è¡¨çš„æ¨¡å‹ï¼‰
    :param table_name: è¡¨å
    :return: å¯¹åº”çš„ORMæ¨¡å‹
    """
    if table_name == "entities":
        from models import Entity  # å‡è®¾ä½ å·²ç»å®šä¹‰äº†ORMæ¨¡å‹
        return Entity
    elif table_name == "relations":
        from models import Relation  # å‡è®¾ä½ å·²ç»å®šä¹‰äº†ORMæ¨¡å‹
        return Relation
    else:
        raise ValueError(f"Unknown table: {table_name}")


# ======================== æ¨¡å‹åŠ è½½ ========================
NER_MODEL_NAME = "uer/roberta-base-finetuned-cluener2020-chinese"

@lru_cache(maxsize=1)
def get_ner_pipeline():
    tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_NAME)
    model = AutoModelForTokenClassification.from_pretrained(NER_MODEL_NAME)
    return pipeline(
        "ner",
        model=model,
        tokenizer=tokenizer,
        aggregation_strategy="first"
    )

@lru_cache(maxsize=1)
def get_re_pipeline():
    return pipeline(
        "text2text-generation",
        model=NER_MODEL_NAME,
        tokenizer=NER_MODEL_NAME,
        max_length=512,
        device=0 if torch.cuda.is_available() else -1
    )


# chatglm_model, chatglm_tokenizer = None, None
# use_chatglm = False
# try:
#     chatglm_model_name = "THUDM/chatglm-6b-int4"
#     chatglm_tokenizer = AutoTokenizer.from_pretrained(chatglm_model_name, trust_remote_code=True)
#     chatglm_model = AutoModel.from_pretrained(
#         chatglm_model_name,
#         trust_remote_code=True,
#         device_map="cpu",
#         torch_dtype=torch.float32
#     ).eval()
#     use_chatglm = True
#     print("âœ… 4-bité‡åŒ–ç‰ˆChatGLMåŠ è½½æˆåŠŸ")
# except Exception as e:
#     print(f"âŒ ChatGLMåŠ è½½å¤±è´¥: {e}")

# ======================== çŸ¥è¯†å›¾è°±ç»“æ„ ========================
knowledge_graph = {"entities": set(), "relations": set()}


# ä¼˜åŒ–çŸ¥è¯†å›¾è°±æ›´æ–°å‡½æ•°ï¼Œå¢åŠ å…¨å±€å˜é‡æ›´æ–°
def update_knowledge_graph(entities, relations):
    """
    æ›´æ–°çŸ¥è¯†å›¾è°±å¹¶ä¿å­˜åˆ°æ•°æ®åº“
    """
    global knowledge_graph  # æ˜ç¡®å£°æ˜ä½¿ç”¨å…¨å±€å˜é‡
    # ä¿å­˜å®ä½“
    for e in entities:
        if isinstance(e, dict) and 'text' in e and 'type' in e:
            save_to_db('entities', {
                'text': e['text'],
                'type': e['type'],
                'start_pos': e.get('start', -1),
                'end_pos': e.get('end', -1),
                'source': 'user_input'
            })
            knowledge_graph["entities"].add((e['text'], e['type']))

    # ä¿å­˜å…³ç³»
    for r in relations:
        if isinstance(r, dict) and all(k in r for k in ("head", "tail", "relation")):
            save_to_db('relations', {
                'head_entity': r['head'],
                'tail_entity': r['tail'],
                'relation_type': r['relation'],
                'source_text': ''  # å¯æ·»åŠ åŸæ–‡å…³è”
            })
            knowledge_graph["relations"].add((r['head'], r['tail'], r['relation']))


# ä¼˜åŒ–çŸ¥è¯†å›¾è°±æ–‡æœ¬æ ¼å¼ç”Ÿæˆå‡½æ•°ï¼Œå¢åŠ æ’åºå’Œå»é‡
def visualize_kg_text():
    """
    ç”ŸæˆçŸ¥è¯†å›¾è°±çš„æ–‡æœ¬æ ¼å¼
    """
    nodes = sorted(set([f"{ent[0]} ({ent[1]})" for ent in knowledge_graph["entities"]]))
    edges = sorted(set([f"{h} --[{r}]-> {t}" for h, t, r in knowledge_graph["relations"]]))
    return "\n".join(["ğŸ“Œ å®ä½“:"] + nodes + ["", "ğŸ“ å…³ç³»:"] + edges)


# ä¼˜åŒ–çŸ¥è¯†å›¾è°±å¯è§†åŒ–å‡½æ•°ï¼ŒåŠ¨æ€ç”ŸæˆHTMLæ–‡ä»¶åï¼Œé¿å…è¦†ç›–
def visualize_kg_interactive(entities, relations):
    """
    ç”Ÿæˆäº¤äº’å¼çš„çŸ¥è¯†å›¾è°±å¯è§†åŒ–
    """
    # åˆ›å»ºä¸€ä¸ªæ–°çš„ç½‘ç»œå›¾
    net = Network(height="700px", width="100%", bgcolor="#ffffff", font_color="black")
    
    # å®šä¹‰å®ä½“ç±»å‹é¢œè‰²
    entity_colors = {
        'PER': '#FF6B6B',  # äººç‰©-çº¢è‰²
        'ORG': '#4ECDC4',  # ç»„ç»‡-é’è‰²
        'LOC': '#45B7D1',  # åœ°ç‚¹-è“è‰²
        'TIME': '#96CEB4', # æ—¶é—´-ç»¿è‰²
        'TITLE': '#D4A5A5' # èŒä½-ç°è‰²
    }
    
    # æ·»åŠ å®ä½“èŠ‚ç‚¹
    for entity in entities:
        node_color = entity_colors.get(entity['type'], '#D3D3D3')  # é»˜è®¤ç°è‰²
        net.add_node(entity['text'], 
                     label=f"{entity['text']} ({entity['type']})",
                     color=node_color,
                     title=f"ç±»å‹: {entity['type']}")
    
    # æ·»åŠ å…³ç³»è¾¹
    for relation in relations:
        net.add_edge(relation['head'], 
                     relation['tail'], 
                     label=relation['relation'], 
                     arrows='to')
    
    # è®¾ç½®ç‰©ç†å¸ƒå±€
    net.set_options('''
    var options = {
        "physics": {
            "forceAtlas2Based": {
                "gravitationalConstant": -50,
                "centralGravity": 0.01,
                "springLength": 100,
                "springConstant": 0.08
            },
            "maxVelocity": 50,
            "solver": "forceAtlas2Based",
            "timestep": 0.35,
            "stabilization": {"iterations": 150}
        }
    }
    ''')
    
    # åŠ¨æ€ç”ŸæˆHTMLæ–‡ä»¶å
    timestamp = int(time.time())
    html_path = f"knowledge_graph_{timestamp}.html"
    net.save_graph(html_path)
    return html_path

# ======================== å®ä½“è¯†åˆ«ï¼ˆNERï¼‰ ========================
def merge_adjacent_entities(entities):
    if not entities:
        return entities

    merged = [entities[0]]
    for entity in entities[1:]:
        last = merged[-1]
        # åˆå¹¶ç›¸é‚»çš„åŒç±»å‹å®ä½“
        if (entity["type"] == last["type"] and
                entity["start"] == last["end"]):
            last["text"] += entity["text"]
            last["end"] = entity["end"]
        else:
            merged.append(entity)

    return merged


def ner(text, model_type="bert"):
    start_time = time.time()

    # å¦‚æœä½¿ç”¨çš„æ˜¯ ChatGLM æ¨¡å‹ï¼Œæ‰§è¡Œ ChatGLM çš„NER
    if model_type == "chatglm" and use_chatglm:
        try:
            prompt = f"""è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­è¯†åˆ«æ‰€æœ‰å®ä½“ï¼Œä¸¥æ ¼æŒ‰ç…§JSONåˆ—è¡¨æ ¼å¼è¿”å›ï¼Œæ¯ä¸ªå®ä½“åŒ…å«textã€typeã€startã€endå­—æ®µï¼š
ç¤ºä¾‹ï¼š[{{"text": "åŒ—äº¬", "type": "LOC", "start": 0, "end": 2}}]
æ–‡æœ¬ï¼š{text}"""
            response = chatglm_model.chat(chatglm_tokenizer, prompt, temperature=0.1)
            if isinstance(response, tuple):
                response = response[0]

            try:
                json_str = re.search(r'\[.*\]', response, re.DOTALL).group()
                entities = json.loads(json_str)
                valid_entities = [ent for ent in entities if all(k in ent for k in ("text", "type", "start", "end"))]
                return valid_entities, time.time() - start_time
            except Exception as e:
                print(f"JSONè§£æå¤±è´¥: {e}")
                return [], time.time() - start_time
        except Exception as e:
            print(f"ChatGLMè°ƒç”¨å¤±è´¥: {e}")
            return [], time.time() - start_time

    # ä½¿ç”¨BERT NER
    text_chunks = [text[i:i + 510] for i in range(0, len(text), 510)]  # å®‰å…¨åˆ†æ®µ
    raw_results = []
    
    # è·å–NER pipeline
    ner_pipeline = get_ner_pipeline()  # ä½¿ç”¨ç¼“å­˜çš„pipeline
    
    for idx, chunk in enumerate(text_chunks):
        chunk_results = ner_pipeline(chunk)  # ä½¿ç”¨è·å–çš„pipeline
        for r in chunk_results:
            r["start"] += idx * 510
            r["end"] += idx * 510
        raw_results.extend(chunk_results)

    entities = [{
        "text": r['word'].replace(' ', ''),
        "start": r['start'],
        "end": r['end'],
        "type": LABEL_MAPPING.get(r.get('entity_group') or r.get('entity'), r.get('entity_group') or r.get('entity'))
    } for r in raw_results]

    entities = merge_adjacent_entities(entities)
    return entities, time.time() - start_time


# ------------------ å®ä½“ç±»å‹æ ‡å‡†åŒ– ------------------
LABEL_MAPPING = {
    "address": "LOC",
    "company": "ORG",
    "name": "PER",
    "organization": "ORG",
    "position": "TITLE",
    "government": "ORG",
    "scene": "LOC",
    "book": "WORK",
    "movie": "WORK",
    "game": "WORK"
}

# æå–å®ä½“
entities, processing_time = ner("Google in New York met Alice")

# æ ‡å‡†åŒ–å®ä½“ç±»å‹
for e in entities:
    e["type"] = LABEL_MAPPING.get(e.get("type"), e.get("type"))

# æ‰“å°æ ‡å‡†åŒ–åçš„å®ä½“
print(f"[DEBUG] æ ‡å‡†åŒ–åå®ä½“åˆ—è¡¨: {[{'text': e['text'], 'type': e['type']} for e in entities]}")

# æ‰“å°å¤„ç†æ—¶é—´
print(f"å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")


# ======================== å…³ç³»æŠ½å–ï¼ˆREï¼‰ ========================
@lru_cache(maxsize=1)
def get_re_pipeline():
    tokenizer = AutoTokenizer.from_pretrained(NER_MODEL_NAME)
    model = AutoModelForTokenClassification.from_pretrained(NER_MODEL_NAME)
    return pipeline(
        "ner",  # ä½¿ç”¨NER pipeline
        model=model,
        tokenizer=tokenizer,
        aggregation_strategy="first"
    )

def re_extract(entities, text, use_bert_model=True):
    if not entities or not text:
        return [], 0
    
    start_time = time.time()
    try:
        # ä½¿ç”¨è§„åˆ™åŒ¹é…å…³ç³»
        relations = []
        
        # å®šä¹‰å…³ç³»å…³é”®è¯å’Œå¯¹åº”çš„å®ä½“ç±»å‹çº¦æŸ
        relation_rules = {
            "ä½äº": {
                "keywords": ["ä½äº", "åœ¨", "åè½äº"],
                "valid_types": {
                    "head": ["ORG", "PER", "LOC"],
                    "tail": ["LOC"]
                }
            },
            "å±äº": {
                "keywords": ["å±äº", "æ˜¯", "ä¸º"],
                "valid_types": {
                    "head": ["ORG", "PER"],
                    "tail": ["ORG", "LOC"]
                }
            },
            "ä»»èŒäº": {
                "keywords": ["ä»»èŒäº", "å°±èŒäº", "å·¥ä½œäº"],
                "valid_types": {
                    "head": ["PER"],
                    "tail": ["ORG"]
                }
            }
        }
        
        # é¢„å¤„ç†å®ä½“ï¼Œå»é™¤é‡å¤å’Œéƒ¨åˆ†åŒ¹é…
        processed_entities = []
        for e in entities:
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰å®ä½“é‡å 
            is_subset = False
            for pe in processed_entities:
                if e["text"] in pe["text"] and e["text"] != pe["text"]:
                    is_subset = True
                    break
            if not is_subset:
                processed_entities.append(e)
        
        # éå†æ–‡æœ¬ä¸­çš„æ¯ä¸ªå¥å­
        sentences = re.split('[ã€‚ï¼ï¼Ÿ.!?]', text)
        for sentence in sentences:
            if not sentence.strip():
                continue
            
            # è·å–å½“å‰å¥å­ä¸­çš„å®ä½“
            sentence_entities = [e for e in processed_entities if e["text"] in sentence]
            
            # æ£€æŸ¥æ¯ä¸ªå…³ç³»ç±»å‹
            for rel_type, rule in relation_rules.items():
                for keyword in rule["keywords"]:
                    if keyword in sentence:
                        # åœ¨å¥å­ä¸­æŸ¥æ‰¾ç¬¦åˆç±»å‹çº¦æŸçš„å®ä½“å¯¹
                        for i, ent1 in enumerate(sentence_entities):
                            for j, ent2 in enumerate(sentence_entities):
                                if i != j:  # é¿å…è‡ªå¾ªç¯
                                    # æ£€æŸ¥å®ä½“ç±»å‹æ˜¯å¦ç¬¦åˆè§„åˆ™
                                    if (ent1["type"] in rule["valid_types"]["head"] and 
                                        ent2["type"] in rule["valid_types"]["tail"]):
                                        # æ£€æŸ¥å®ä½“åœ¨å¥å­ä¸­çš„ä½ç½®å…³ç³»
                                        if sentence.find(ent1["text"]) < sentence.find(ent2["text"]):
                                            relations.append({
                                                "head": ent1["text"],
                                                "tail": ent2["text"],
                                                "relation": rel_type
                                            })
        
        # å»é‡
        unique_relations = []
        seen = set()
        for rel in relations:
            rel_key = (rel["head"], rel["tail"], rel["relation"])
            if rel_key not in seen:
                seen.add(rel_key)
                unique_relations.append(rel)
        
        return unique_relations, time.time() - start_time
            
    except Exception as e:
        logging.error(f"å…³ç³»æŠ½å–å¤±è´¥: {e}")
        return [], time.time() - start_time


# ======================== æ–‡æœ¬åˆ†æä¸»æµç¨‹ ========================
def create_knowledge_graph(entities, relations):
    """
    åˆ›å»ºçŸ¥è¯†å›¾è°±å¯è§†åŒ–ï¼ˆæ–‡æœ¬æ ¼å¼ï¼‰
    """
    # è®¾ç½®å®ä½“ç±»å‹çš„é¢œè‰²æ˜ å°„
    entity_colors = {
        'PER': 'ğŸ”´',  # äººç‰©-çº¢è‰²
        'ORG': 'ğŸ”µ',  # ç»„ç»‡-è“è‰²
        'LOC': 'ğŸŸ¢',  # åœ°ç‚¹-ç»¿è‰²
        'TIME': 'ğŸŸ¡', # æ—¶é—´-é»„è‰²
        'TITLE': 'ğŸŸ£' # èŒä½-ç´«è‰²
    }
    
    # ç”Ÿæˆå®ä½“åˆ—è¡¨
    entity_list = []
    for entity in entities:
        emoji = entity_colors.get(entity['type'], 'âšª')
        entity_list.append(f"{emoji} {entity['text']} ({entity['type']})")
    
    # ç”Ÿæˆå…³ç³»åˆ—è¡¨
    relation_list = []
    for relation in relations:
        relation_list.append(f"{relation['head']} --[{relation['relation']}]--> {relation['tail']}")
    
    # ç”ŸæˆHTMLå†…å®¹
    html_content = f"""
    <div style="font-family: Arial, sans-serif; padding: 20px;">
        <h3 style="color: #333; margin-bottom: 15px;">ğŸ“Œ å®ä½“åˆ—è¡¨ï¼š</h3>
        <div style="margin-bottom: 20px;">
            {chr(10).join(f'<div style="margin: 5px 0;">{entity}</div>' for entity in entity_list)}
        </div>
        
        <h3 style="color: #333; margin-bottom: 15px;">ğŸ“ å…³ç³»åˆ—è¡¨ï¼š</h3>
        <div>
            {chr(10).join(f'<div style="margin: 5px 0;">{relation}</div>' for relation in relation_list)}
        </div>
        
        <div style="margin-top: 20px; padding: 10px; background-color: #f8f9fa; border-radius: 5px;">
            <h4 style="color: #666; margin-bottom: 10px;">å›¾ä¾‹è¯´æ˜ï¼š</h4>
            <div style="display: flex; gap: 15px; flex-wrap: wrap;">
                {chr(10).join(f'<div style="display: flex; align-items: center; gap: 5px;"><span>{emoji}</span><span>{label}</span></div>' for label, emoji in entity_colors.items())}
            </div>
        </div>
    </div>
    """
    
    return html_content

def process_text(text, model_type="bert"):
    """
    å¤„ç†æ–‡æœ¬ï¼Œè¿›è¡Œå®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–
    """
    start_time = time.time()
    
    # å®ä½“è¯†åˆ«
    entities, ner_duration = ner(text, model_type)
    if not entities:
        return "", "", "", f"{time.time() - start_time:.2f} ç§’"
    
    # å…³ç³»æŠ½å–
    relations, re_duration = re_extract(entities, text)
    
    # ç”Ÿæˆæ–‡æœ¬æ ¼å¼çš„å®ä½“å’Œå…³ç³»æè¿°
    ent_text = "ğŸ“Œ å®ä½“:\n" + "\n".join([f"{e['text']} ({e['type']})" for e in entities])
    rel_text = "\n\nğŸ“ å…³ç³»:\n" + "\n".join([f"{r['head']} --[{r['relation']}]--> {r['tail']}" for r in relations])
    
    # ç”ŸæˆçŸ¥è¯†å›¾è°±
    kg_text = create_knowledge_graph(entities, relations)
    
    total_duration = time.time() - start_time
    return ent_text, rel_text, kg_text, f"{total_duration:.2f} ç§’"

# ======================== çŸ¥è¯†å›¾è°±å¯è§†åŒ– ========================
def generate_kg_image(entities, relations):
    """
    ç”ŸæˆçŸ¥è¯†å›¾è°±çš„å›¾ç‰‡å¹¶ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ï¼ˆHugging Faceé€‚é…ç‰ˆï¼‰
    """
    try:
        import tempfile
        import matplotlib.pyplot as plt
        import networkx as nx
        import os

        # === 1. å¼ºåˆ¶è®¾ç½®ä¸­æ–‡å­—ä½“ ===
        plt.rcParams['font.sans-serif'] = ['Noto Sans CJK SC']  # Hugging Faceå†…ç½®å­—ä½“
        plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜

        # === 2. æ£€æŸ¥è¾“å…¥æ•°æ® ===
        if not entities or not relations:
            return None

        # === 3. åˆ›å»ºå›¾è°± ===
        G = nx.DiGraph()
        entity_colors = {
            'PER': '#FF6B6B',  # çº¢è‰²
            'ORG': '#4ECDC4',  # é’è‰²
            'LOC': '#45B7D1',  # è“è‰²
            'TIME': '#96CEB4', # ç»¿è‰²
            'TITLE': '#D4A5A5' # ç°è‰²
        }

        # æ·»åŠ èŠ‚ç‚¹ï¼ˆå®ä½“ï¼‰
        for entity in entities:
            G.add_node(
                entity["text"],
                label=f"{entity['text']} ({entity['type']})",
                color=entity_colors.get(entity['type'], '#D3D3D3')
            )

        # æ·»åŠ è¾¹ï¼ˆå…³ç³»ï¼‰
        for relation in relations:
            if relation["head"] in G.nodes and relation["tail"] in G.nodes:
                G.add_edge(
                    relation["head"],
                    relation["tail"],
                    label=relation["relation"]
                )

        # === 4. ç»˜å›¾é…ç½® ===
        plt.figure(figsize=(12, 8), dpi=150)  # é™ä½DPIä»¥èŠ‚çœå†…å­˜
        pos = nx.spring_layout(G, k=0.7, seed=42)  # å›ºå®šéšæœºç§å­ä¿è¯å¸ƒå±€ç¨³å®š

        # ç»˜åˆ¶èŠ‚ç‚¹å’Œè¾¹
        nx.draw_networkx_nodes(
            G, pos,
            node_color=[G.nodes[n]['color'] for n in G.nodes],
            node_size=800
        )
        nx.draw_networkx_edges(
            G, pos,
            edge_color='#888888',
            width=1.5,
            arrows=True,
            arrowsize=20
        )

        # === 5. ç»˜åˆ¶ä¸­æ–‡æ ‡ç­¾ï¼ˆå…³é”®ä¿®æ”¹ç‚¹ï¼‰===
        nx.draw_networkx_labels(
            G, pos,
            labels={n: G.nodes[n]['label'] for n in G.nodes},
            font_size=10,
            font_family='Noto Sans CJK SC'  # æ˜¾å¼æŒ‡å®šå­—ä½“
        )
        nx.draw_networkx_edge_labels(
            G, pos,
            edge_labels=nx.get_edge_attributes(G, 'label'),
            font_size=8,
            font_family='Noto Sans CJK SC'  # æ˜¾å¼æŒ‡å®šå­—ä½“
        )

        plt.axis('off')
        
        # === 6. ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ ===
        temp_dir = tempfile.mkdtemp()
        file_path = os.path.join(temp_dir, "kg.png")
        plt.savefig(file_path, bbox_inches='tight')
        plt.close()
        
        return file_path
        
    except Exception as e:
        logging.error(f"ç”ŸæˆçŸ¥è¯†å›¾è°±å›¾ç‰‡å¤±è´¥: {str(e)}")
        return None


def process_file(file, model_type="bert"):
    try:
        with open(file.name, 'rb') as f:
            content = f.read()

        if len(content) > 5 * 1024 * 1024:
            return "âŒ æ–‡ä»¶å¤ªå¤§", "", "", "", None

        # æ£€æµ‹ç¼–ç 
        try:
            encoding = chardet.detect(content)['encoding'] or 'utf-8'
            text = content.decode(encoding)
        except UnicodeDecodeError:
            # å°è¯•å¸¸è§ä¸­æ–‡ç¼–ç 
            for enc in ['gb18030', 'utf-16', 'big5']:
                try:
                    text = content.decode(enc)
                    break
                except:
                    continue
            else:
                return "âŒ ç¼–ç è§£æå¤±è´¥", "", "", "", None

        # è°ƒç”¨ç°æœ‰æµç¨‹å¤„ç†æ–‡æœ¬
        ent_text, rel_text, kg_text, duration = process_text(text, model_type)
        
        # ç”ŸæˆçŸ¥è¯†å›¾è°±å›¾ç‰‡
        entities, _ = ner(text, model_type)
        relations, _ = re_extract(entities, text)
        kg_image_path = generate_kg_image(entities, relations)  # è¿”å›æ–‡ä»¶è·¯å¾„
        
        return ent_text, rel_text, kg_text, duration, kg_image_path
        
    except Exception as e:
        logging.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯: {str(e)}")
        return f"âŒ æ–‡ä»¶å¤„ç†é”™è¯¯: {str(e)}", "", "", "", None


# ======================== æ¨¡å‹è¯„ä¼°ä¸è‡ªåŠ¨æ ‡æ³¨ ========================
def convert_telegram_json_to_eval_format(path):
    with open(path, encoding="utf-8") as f:
        data = json.load(f)
    if isinstance(data, dict) and "text" in data:
        return [{"text": data["text"], "entities": [
            {"text": data["text"][e["start"]:e["end"]]} for e in data.get("entities", [])
        ]}]
    elif isinstance(data, list):
        return data
    elif isinstance(data, dict) and "messages" in data:
        result = []
        for m in data.get("messages", []):
            if isinstance(m.get("text"), str):
                result.append({"text": m["text"], "entities": []})
            elif isinstance(m.get("text"), list):
                txt = ''.join([x["text"] if isinstance(x, dict) else x for x in m["text"]])
                result.append({"text": txt, "entities": []})
        return result
    return []


def evaluate_ner_model(data, model_type):
    tp, fp, fn = 0, 0, 0
    POS_TOLERANCE = 1

    for item in data:
        text = item["text"]
        # å¤„ç†æ ‡æ³¨æ•°æ®
        gold_entities = validate_gold_entities([
            {
                "text": e["text"],
                "type": LABEL_MAPPING.get(e["type"], e["type"]),
                "start": e.get("start", -1),
                "end": e.get("end", -1)
            }
            for e in item.get("entities", [])
        ])

        # è·å–é¢„æµ‹ç»“æœ
        pred_entities, _ = ner(text, model_type)

        # åˆå§‹åŒ–åŒ¹é…çŠ¶æ€
        matched_gold = [False] * len(gold_entities)
        matched_pred = [False] * len(pred_entities)

        # éå†é¢„æµ‹å®ä½“å¯»æ‰¾åŒ¹é…
        for p_idx, p in enumerate(pred_entities):
            for g_idx, g in enumerate(gold_entities):
                if not matched_gold[g_idx] and \
                        p["text"] == g["text"] and \
                        p["type"] == g["type"] and \
                        abs(p["start"] - g["start"]) <= POS_TOLERANCE and \
                        abs(p["end"] - g["end"]) <= POS_TOLERANCE:
                    matched_gold[g_idx] = True
                    matched_pred[p_idx] = True
                    break

        # ç»Ÿè®¡æŒ‡æ ‡
        tp += sum(matched_pred)
        fp += len(pred_entities) - sum(matched_pred)
        fn += len(gold_entities) - sum(matched_gold)

    # å¤„ç†é™¤é›¶æƒ…å†µ
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    return (f"Precision: {precision:.2f}\n"
            f"Recall: {recall:.2f}\n"
            f"F1: {f1:.2f}")


def auto_annotate(file, model_type):
    data = convert_telegram_json_to_eval_format(file.name)
    for item in data:
        ents, _ = ner(item["text"], model_type)
        item["entities"] = ents
    return json.dumps(data, ensure_ascii=False, indent=2)


def save_json(json_text):
    fname = f"auto_labeled_{int(time.time())}.json"
    with open(fname, "w", encoding="utf-8") as f:
        f.write(json_text)
    return fname


# ======================== æ•°æ®é›†å¯¼å…¥ ========================
def import_dataset(path="D:/äº‘è¾¹æ™ºç®—/æš—è¯­è¯†åˆ«/filtered_results"):
    import os
    import json

    for filename in os.listdir(path):
        if filename.endswith('.json'):
            filepath = os.path.join(path, filename)
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
                # è°ƒç”¨ç°æœ‰å¤„ç†æµç¨‹
                process_text(data['text'])
                print(f"å·²å¤„ç†æ–‡ä»¶: {filename}")


# ======================== Gradio ç•Œé¢ ========================
with gr.Blocks(css="""
    .kg-graph {height: 700px; overflow-y: auto;}
    .warning {color: #ff6b6b;}
    .error {color: #ff0000; padding: 10px; background-color: #ffeeee; border-radius: 5px;}
""") as demo:
    gr.Markdown("# ğŸ¤– èŠå¤©è®°å½•å®ä½“å…³ç³»è¯†åˆ«ç³»ç»Ÿ")

    with gr.Tab("ğŸ“„ æ–‡æœ¬åˆ†æ"):
        input_text = gr.Textbox(lines=6, label="è¾“å…¥æ–‡æœ¬")
        model_type = gr.Radio(["bert", "chatglm"], value="bert", label="é€‰æ‹©æ¨¡å‹")
        btn = gr.Button("å¼€å§‹åˆ†æ")
        out1 = gr.Textbox(label="è¯†åˆ«å®ä½“")
        out2 = gr.Textbox(label="è¯†åˆ«å…³ç³»")
        out3 = gr.HTML(label="çŸ¥è¯†å›¾è°±")  # ä½¿ç”¨HTMLç»„ä»¶æ˜¾ç¤ºæ–‡æœ¬æ ¼å¼çš„çŸ¥è¯†å›¾è°±
        out4 = gr.Textbox(label="è€—æ—¶")
        btn.click(fn=process_text, inputs=[input_text, model_type], outputs=[out1, out2, out3, out4])

    with gr.Tab("ğŸ—‚ æ–‡ä»¶åˆ†æ"):
        file_input = gr.File(file_types=[".txt", ".json"])
        file_btn = gr.Button("ä¸Šä¼ å¹¶åˆ†æ")
        fout1, fout2, fout3, fout4, fout5 = gr.Textbox(), gr.Textbox(), gr.Textbox(), gr.Textbox(), gr.File(label="ä¸‹è½½çŸ¥è¯†å›¾è°±å›¾ç‰‡")
        file_btn.click(fn=process_file, inputs=[file_input, model_type], outputs=[fout1, fout2, fout3, fout4, fout5])

    with gr.Tab("ğŸ“Š æ¨¡å‹è¯„ä¼°"):
        eval_file = gr.File(label="ä¸Šä¼ æ ‡æ³¨ JSON")
        eval_model = gr.Radio(["bert", "chatglm"], value="bert")
        eval_btn = gr.Button("å¼€å§‹è¯„ä¼°")
        eval_output = gr.Textbox(label="è¯„ä¼°ç»“æœ", lines=5)
        eval_btn.click(lambda f, m: evaluate_ner_model(convert_telegram_json_to_eval_format(f.name), m),
                       [eval_file, eval_model], eval_output)

    with gr.Tab("âœï¸ è‡ªåŠ¨æ ‡æ³¨"):
        raw_file = gr.File(label="ä¸Šä¼  Telegram åŸå§‹ JSON")
        auto_model = gr.Radio(["bert", "chatglm"], value="bert")
        auto_btn = gr.Button("è‡ªåŠ¨æ ‡æ³¨")
        marked_texts = gr.Textbox(label="æ ‡æ³¨ç»“æœ", lines=20)
        download_btn = gr.Button("ğŸ’¾ ä¸‹è½½æ ‡æ³¨æ–‡ä»¶")
        auto_btn.click(fn=auto_annotate, inputs=[raw_file, auto_model], outputs=marked_texts)
        download_btn.click(fn=save_json, inputs=marked_texts, outputs=gr.File())

    with gr.Tab("ğŸ“‚ æ•°æ®ç®¡ç†"):
        gr.Markdown("### æ•°æ®é›†å¯¼å…¥")
        dataset_path = gr.Textbox(
            value="D:/äº‘è¾¹æ™ºç®—/æš—è¯­è¯†åˆ«/filtered_results",
            label="æ•°æ®é›†è·¯å¾„"
        )
        import_btn = gr.Button("å¯¼å…¥æ•°æ®é›†åˆ°æ•°æ®åº“")
        import_output = gr.Textbox(label="å¯¼å…¥æ—¥å¿—")
        import_btn.click(fn=lambda: import_dataset(dataset_path.value), outputs=import_output)

demo.launch(server_name="0.0.0.0", server_port=7860)